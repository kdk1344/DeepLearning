{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing  \n",
    "\n",
    "* intent Classification 테스크를 위한 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khj_a\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\package\\_directory_reader.py:17: UserWarning: Failed to initialize NumPy: numpy.core.multiarray failed to import (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16260/2044474051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "from src.dataset import Preprocessing\n",
    "from src.model import EpochLogger, MakeEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\khj_a\\AppData\\Local\\Programs\\Python\\Python38\\Scripts\\pip.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_internal\\cli\\main.py\", line 73, in main\n",
      "    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_internal\\commands\\__init__.py\", line 104, in create_command\n",
      "    module = importlib.import_module(module_path)\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 17, in <module>\n",
      "    from pip._internal.cli.req_command import RequirementCommand, with_cleanup\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 16, in <module>\n",
      "    from pip._internal.index.collector import LinkCollector\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_internal\\index\\collector.py\", line 14, in <module>\n",
      "    from pip._vendor import html5lib, requests\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_vendor\\requests\\__init__.py\", line 43, in <module>\n",
      "    from pip._vendor import urllib3\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_vendor\\urllib3\\__init__.py\", line 7, in <module>\n",
      "    from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_vendor\\urllib3\\connectionpool.py\", line 39, in <module>\n",
      "    from .response import HTTPResponse\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 156, in <module>\n",
      "    class HTTPResponse(io.IOBase):\n",
      "  File \"c:\\users\\khj_a\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 389, in HTTPResponse\n",
      "    DECODER_ERROR_CLASSES += (brotli.error,)\n",
      "AttributeError: module 'brotli' has no attribute 'error'\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.intent_label_dir = \"./data/dataset/intent_label.json\"\n",
    "        self.intent_data_dir = \"./data/dataset/intent_data.csv\"\n",
    "        \n",
    "        self.intent_label = self.load_intent_label()\n",
    "        self.prep = Preprocessing()\n",
    "\n",
    "    def load_intent_label(self):\n",
    "        f = open(self.intent_label_dir, encoding=\"UTF-8\") \n",
    "        intent_label = json.loads(f.read())\n",
    "        self.intents = list(intent_label.keys())\n",
    "        return intent_label\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return sentence.split()\n",
    "    \n",
    "    def tokenize_dataset(self, dataset):\n",
    "        token_dataset = []\n",
    "        for data in dataset:\n",
    "            token_dataset.append(self.tokenize(data))\n",
    "        return token_dataset\n",
    "\n",
    "    def make_intent_dataset(self, embed): # intent 분류를 위한 Dataset 생성\n",
    "        intent_dataset = pd.read_csv(self.intent_data_dir) # 데이터를 로드\n",
    "\n",
    "        labels = [self.intent_label[label] for label in intent_dataset[\"label\"].to_list()] # label\n",
    "        \n",
    "        # 사용자 발화 : 사용자가 챗봇과 커뮤니케이션하기 위해 내뱉는 말 또는 텍스트등을 의미\n",
    "        intent_querys = self.tokenize_dataset(intent_dataset[\"question\"].tolist()) # 사용자 발화 tokenize\n",
    "        \n",
    "        dataset = list(zip(intent_querys, labels)) # (사용자 발화, intent) 형태로 가공\n",
    "        intent_train_dataset, intent_test_dataset = self.word2idx_dataset(dataset, embed) # word2index\n",
    "        return intent_train_dataset, intent_test_dataset\n",
    "    \n",
    "    def word2idx_dataset(self, dataset ,embed, train_ratio = 0.8):\n",
    "        embed_dataset = []\n",
    "        question_list, label_list = [], []\n",
    "        flag = True\n",
    "        random.shuffle(dataset) # 훈련용과 검증용으로 나눌 때 intent 편형이 나타나지 않도록 데이터를 셔플\n",
    "        for query, label in dataset :\n",
    "            q_vec = embed.query2idx(query) # 사용자 발화 index 형태로 변환\n",
    "            q_vec = self.prep.pad_idx_sequencing(q_vec) # 사용자 발화 최대길이까지 padding\n",
    "\n",
    "            question_list.append(torch.tensor([q_vec]))\n",
    "            label_list.append(torch.tensor([label]))\n",
    "\n",
    "        x = torch.cat(question_list)\n",
    "        y = torch.cat(label_list)\n",
    "\n",
    "        # 학습용과 검증용으로 나누기\n",
    "        x_len = x.size()[0]\n",
    "        y_len = y.size()[0]\n",
    "        if(x_len == y_len):\n",
    "            train_size = int(x_len*train_ratio)\n",
    "            \n",
    "            train_x = x[:train_size]\n",
    "            train_y = y[:train_size]\n",
    "\n",
    "            test_x = x[train_size+1:]\n",
    "            test_y = y[train_size+1:]\n",
    "            # TensorDataset으로 감싸기\n",
    "            # 파이토치의 TensorDatset은 tensor를 감싸는 Dataset\n",
    "            # 인덱싱 방식과 길이를 정의함으로써 tensor의 첫 번째 차원을 따라 반복, 인덱스, 슬라이스를 위한 방법을 제공\n",
    "            # 학습할 때 동일한 라이엔서 독립 변수와 종속 변수에 쉽게 접근할 수 있음\n",
    "            train_dataset = TensorDataset(train_x,train_y)\n",
    "            test_dataset = TensorDataset(test_x,test_y)\n",
    "            \n",
    "            return train_dataset, test_dataset\n",
    "            \n",
    "        else:\n",
    "            print(\"ERROR x!=y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MakeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dataset = pd.read_csv(dataset.intent_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dataset.groupby(['label']).count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = MakeEmbed()\n",
    "embed.load_word2vec()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "intent_train_dataset, intent_test_dataset = dataset.make_intent_dataset(embed)\n",
    "\n",
    "train_dataloader = DataLoader(intent_train_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "test_dataloader = DataLoader(intent_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_train_dataset.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Sentence Classification\n",
    "### tensorflow code : https://github.com/SeonbeomKim/TensorFlow-TextCNN/blob/master/TextCNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, w2v, dim, kernels, dropout, num_class):\n",
    "        super(textCNN, self).__init__()\n",
    "        # Word2vec으로 미리 학습해둔 임베딩 적용\n",
    "        vocab_size = w2v.size()[0]\n",
    "        emb_dim = w2v.size()[1]\n",
    "        self.embed = nn.Embedding(vocab_size+2, emb_dim)\n",
    "        self.embed.weight[2:].data.copy_(w2v)\n",
    "        # self.embed.weight.requires_grad = False # 임베딩 레이어 학습 유무\n",
    "        \n",
    "        # 윈도우 사이즈가 다른 각각의 conv layer 를 nn.ModuleList로 저장\n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim, (w, emb_dim)) for w in kernels])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(len(kernels)*dim, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "\n",
    "        con_x = [conv(emb_x) for conv in self.convs] # 각 사이즈 별 결과를 list로 저장\n",
    "        # [(out_channels, conv결과 길이), ...]\n",
    "\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x] # 각 사이즈별 max_pool결과 저장\n",
    "        # [(256, 1), ...]\n",
    "\n",
    "        fc_x = torch.cat(pool_x, dim=1) # concat 하여 fc layer의 입력 형태로 만듬\n",
    "\n",
    "        fc_x = fc_x.squeeze(-1) # 차원 맞추기\n",
    "        fc_x = self.dropout(fc_x) \n",
    "        logit = self.fc(fc_x)\n",
    "        return logit\n",
    "\n",
    "# 모델의 가중치 저장을 위한 코드\n",
    "def save(model, save_dir, save_prefix, epoch):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, epoch)\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = embed.word2vec.wv.vectors\n",
    "weights = torch.FloatTensor(weights)\n",
    "\n",
    "num_class = len(dataset.intent_label) \n",
    "model = textCNN(weights, 256, [3,4,5], 0.5, num_class)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epoch = 10\n",
    "prev_acc = 0\n",
    "save_dir = \"./data/pretraining/1_intent_clsf_model/\"\n",
    "save_prefix = \"intent_clsf\"\n",
    "for i in range(epoch):\n",
    "    steps = 0\n",
    "    model.train() \n",
    "    #for data in train_dataloader:\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "            logit = model.forward(x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(logit, target) # loass function\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= accuracy.numpy())\n",
    "    \n",
    "    model.eval() # weight 업데이트 금지\n",
    "    steps = 0\n",
    "    accuarcy_list = []\n",
    "    #for data in test_dataloader:\n",
    "    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "\n",
    "            logit = model.forward(x)\n",
    "            loss = F.cross_entropy(logit, target)\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            accuarcy_list.append(accuracy.tolist())\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n",
    "    \n",
    "    acc = sum(accuarcy_list)/len(accuarcy_list)\n",
    "    if(acc>prev_acc):\n",
    "        prev_acc = acc\n",
    "        save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./data/pretraining/save/1_intent_clsf_model/intent_clsf_97.217_steps_33.pt\"))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "q = \"제주도 오늘 날씨 알려줘\"\n",
    "\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "intent = dataset.intents[torch.argmax(f).tolist()]\n",
    "\n",
    "print(\"발화 : \" + q)\n",
    "print(\"의도 : \" + intent)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ceb9ecc5f5f802cb05537ec1ae1602a5cb641d69ad11cd1f339e5a39af141e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
